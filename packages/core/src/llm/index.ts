import { ChatMessage, PromptLog, PromptTemplate } from "../types/chat.type";
import {
  CLLM,
  CompletionOptions,
  LLMOptions,
  ModelProvider,
} from "../types/config.type";
import mergeJson from "../utils/merge";
import {
  autodetectPromptTemplate,
  autodetectTemplateFunction,
  autodetectTemplateType,
} from "./autodetect";
import { DEFAULT_CONTEXT_LENGTH, DEFAULT_MAX_TOKENS } from "./constants";
import Handlebars from "handlebars";

export class BaseLLM implements CLLM {
  private _llmOptions: LLMOptions;
  model: string;
  title?: string;
  url?: string;
  completionOptions: CompletionOptions;
  contentLength: number;
  promptTemplates?: Record<string, PromptTemplate>;

  static providerName: ModelProvider;
  get providerName(): ModelProvider {
    return (this.constructor as typeof BaseLLM).providerName;
  }

  constructor(options: LLMOptions) {
    this._llmOptions = options;
    this.model = options.model;
    this.title = options.title;
    this.url = options.url;
    this.contentLength =
      options.completionOptions?.options?.num_ctx || DEFAULT_CONTEXT_LENGTH;
    this.completionOptions = {
      ...options.completionOptions,
      model: options.model || "gpt-4",
      options: {
        ...options.completionOptions?.options,
        num_predict:
          options.completionOptions?.options?.num_predict || DEFAULT_MAX_TOKENS,
      },
    };

    const templateType = options.template ?? autodetectTemplateType(this.model);

    this.promptTemplates = {
      ...autodetectPromptTemplate(this.model, templateType),
      ...options.promptTemplates,
    };
  }

  async *streamComplete(
    _prompt: string,
    options: CompletionOptions,
  ): AsyncGenerator<string> {
    const { completionOptions } = this._parseCompletionOptions(options);

    let completion = "";
    for await (const chunk of this._streamComplete(
      _prompt,
      completionOptions,
    )) {
      completion += chunk;
      yield chunk;
    }
    return {
      completion,
    };
  }

  protected async *_streamComplete(
    _prompt: string,
    options: CompletionOptions,
  ): AsyncGenerator<string> {
    throw new Error("_streamComplete Not implemented");
  }

  protected async *_streamChat(
    messages: ChatMessage[],
    completionOptions: CompletionOptions = {},
    cancelToken?: AbortSignal,
  ): AsyncGenerator<ChatMessage> {
    throw new Error("_streamChat Not implemented");
  }

  async *streamChat(
    messages: ChatMessage[],
    options: CompletionOptions = {},
    cancelToken?: AbortSignal,
  ): AsyncGenerator<ChatMessage, PromptLog> {
    const { completionOptions } = this._parseCompletionOptions(options);
    const prompt = this._formatChatMessages(messages);
    let completion = "";
    try {
      for await (const chunk of this._streamChat(
        messages,
        completionOptions,
        cancelToken,
      )) {
        completion += chunk;
        yield chunk;
      }
    } catch (error) {
      console.log("ğŸš€ ~ BaseLLM ~ streamChat error:", error);
      throw error;
    }
    return {
      prompt,
      completion,
      modelTitle: this.title ?? completionOptions.model ?? "",
      completionOptions,
    };
  }

  getModel(): string {
    return this.model;
  }

  private _formatChatMessages(messages: ChatMessage[]): string {
    const messagesCopy = messages ? messages.map((v) => ({ ...v })) : [];
    let formatStr = "";
    for (const msg of messagesCopy) {
      formatStr += `<${msg.role}>\n${msg.content || ""}\n\n`;
    }

    return formatStr;
  }
  /**
   * åˆ¤æ–­å½“å‰æœåŠ¡æ˜¯å¦æ”¯æŒè¡¥å…¨åŠŸèƒ½
   * @returns å¦‚æœæ”¯æŒè¡¥å…¨åŠŸèƒ½åˆ™è¿”å›trueï¼Œå¦åˆ™è¿”å›false
   */
  supportsCompletions(): boolean {
    if (["deepseek"].includes(this.providerName)) {
      return false;
    }
    return true;
  }
  /**
   * åˆ¤æ–­å½“å‰provideræ˜¯å¦æ”¯æŒé¢„å¡«å……åŠŸèƒ½
   * @returns å¦‚æœå½“å‰provideræ”¯æŒé¢„å¡«å……åŠŸèƒ½ï¼Œåˆ™è¿”å›trueï¼›å¦åˆ™è¿”å›false
   */
  supportsPrefill(): boolean {
    return ["ollama"].includes(this.providerName);
  }
  /**
   * æ¸²æŸ“æç¤ºæ¨¡æ¿
   *
   * @param template æç¤ºæ¨¡æ¿ï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–è€…å‡½æ•°
   * @param history å†å²èŠå¤©è®°å½•æ•°ç»„
   * @param otherData å…¶ä»–æ•°æ®ï¼Œç±»å‹ä¸ºé”®å€¼å¯¹
   * @param canPutWordsInModelsMouth æ˜¯å¦å…è®¸å°†æ–‡å­—æ”¾å…¥æ¨¡å‹å£ä¸­ï¼ˆé»˜è®¤ä¸ºfalseï¼‰
   * @returns å­—ç¬¦ä¸²æˆ–èŠå¤©æ¶ˆæ¯æ•°ç»„ï¼Œå…·ä½“å–å†³äºæ¨¡æ¿ç±»å‹å’Œæ‰§è¡Œç»“æœ
   */
  public renderPromptTemplate(
    template: PromptTemplate,
    history: ChatMessage[],
    otherData: Record<string, string>,
  ) {
    if (typeof template === "string") {
      const data: any = {
        history: history,
        ...otherData,
      };

      const compiledTemplate = Handlebars.compile(template);
      return compiledTemplate(data);
    }

    const rendered = template(history, {
      ...otherData,
      supportsCompletions: this.supportsCompletions() ? "true" : "false",
      supportsPrefill: this.supportsPrefill() ? "true" : "false",
    });

    if (
      typeof rendered !== "string" &&
      rendered[rendered.length - 1].role === "assistant"
    ) {
      const templateMessages = autodetectTemplateFunction(
        this.model,
        this.providerName,
        autodetectTemplateType(this.model),
      );
      return templateMessages(rendered);
    }

    return rendered;
  }
  private _parseCompletionOptions(options: CompletionOptions) {
    const raw = options.raw ?? false;

    const completionOptions: CompletionOptions = mergeJson(
      this.completionOptions,
      options,
    );

    return { completionOptions, raw };
  }
}
